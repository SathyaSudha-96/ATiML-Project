{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csvFileName  = 'C:\\\\Users\\\\sathy\\\\Desktop\\\\ATiML_Project\\\\master996.csv'\n",
    "data = pd.read_csv ( open ( csvFileName ), delimiter=';',encoding= 'unicode_escape')\n",
    "\n",
    "#Extracting the bookid into Dataframe column FileNo, book id pg10067- FileNo - 10067\n",
    "data['FileNo'] = data['book_id'].str.replace(r'\\D+', '').astype(int)\n",
    "\n",
    "#Sorting the books based on FileNo\n",
    "data.sort_values(by=['FileNo'], inplace=True)\n",
    "\n",
    "import glob\n",
    "\n",
    "# Getting all files and its names into dict and then converting to dataframe\"\"\"\n",
    "Content = {}\n",
    "\n",
    "#Its placed in Books folder at local , need to change it later \n",
    "%timeit\n",
    "files = glob.glob(\"C:\\\\Users\\\\sathy\\\\Desktop\\\\ATiML_Project\\\\Gutenberg_English_Fiction_1k\\\\Books\\\\*.html\")\n",
    "for f in files:\n",
    "    with open(f,mode ='r', encoding = 'utf8') as myfile:\n",
    "        Content[f]=myfile.read().replace(\"<br>\", '\\n')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>&lt;p&gt;It was Carnival time in the ancient and onc...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>&lt;p&gt;A king\\n&lt;p&gt;Upon whose property...\\n&lt;p&gt;A dam...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>&lt;p&gt;Edition: 10\\n&lt;p&gt;This eBook was produced by ...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>&lt;p&gt;Her Fancy and His Fact\\n&lt;p&gt;The old by-road ...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>&lt;p&gt;A solitary room at midnight: a single wax c...</td>\n",
       "      <td>Detective and Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>&lt;p&gt;To Sir George Hamilton Seymour, G.C.H.\\n&lt;p&gt;...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>&lt;p&gt;It was a cold raw evening in February as I ...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>&lt;p&gt;As there appeared to be but little prospect...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>&lt;p&gt;The demand for Wee Wifie has led to a reiss...</td>\n",
       "      <td>Detective and Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>&lt;p&gt;A Novel\\n&lt;p&gt;A Novel.\\n&lt;p&gt;Mrs. HENRY WOOD,\\n...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>&lt;p&gt;\"But, Dolly! father will never give his con...</td>\n",
       "      <td>Western Stories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>&lt;p&gt;London; Printed by A. Spottiswoode, New-Str...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>&lt;p&gt;No. 198 EAGLE SERIES\\n&lt;p&gt;This famous line w...</td>\n",
       "      <td>Sea and Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>&lt;p&gt;The Works of E. P. Roe\\n&lt;p&gt;In sending this,...</td>\n",
       "      <td>Detective and Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>&lt;p&gt;'It's very good of you to have met me, Ambr...</td>\n",
       "      <td>Detective and Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>&lt;p&gt;\"The Forsyte Saga\" was the title originally...</td>\n",
       "      <td>Humorous and Wit and Satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>&lt;p&gt;Edition: 10\\n&lt;p&gt;Juliet Sutherland, Charles ...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>&lt;p&gt;E-text prepared by Mardi Desjardins, Stephe...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>&lt;p&gt;A list of the changes made can be found at ...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>&lt;p&gt;It was Sunday evening, and on Sundays Max S...</td>\n",
       "      <td>Literary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Data  \\\n",
       "961  <p>It was Carnival time in the ancient and onc...   \n",
       "962  <p>A king\\n<p>Upon whose property...\\n<p>A dam...   \n",
       "995  <p>Edition: 10\\n<p>This eBook was produced by ...   \n",
       "958  <p>Her Fancy and His Fact\\n<p>The old by-road ...   \n",
       "959  <p>A solitary room at midnight: a single wax c...   \n",
       "967  <p>To Sir George Hamilton Seymour, G.C.H.\\n<p>...   \n",
       "968  <p>It was a cold raw evening in February as I ...   \n",
       "969  <p>As there appeared to be but little prospect...   \n",
       "376  <p>The demand for Wee Wifie has led to a reiss...   \n",
       "741  <p>A Novel\\n<p>A Novel.\\n<p>Mrs. HENRY WOOD,\\n...   \n",
       "834  <p>\"But, Dolly! father will never give his con...   \n",
       "841  <p>London; Printed by A. Spottiswoode, New-Str...   \n",
       "862  <p>No. 198 EAGLE SERIES\\n<p>This famous line w...   \n",
       "988  <p>The Works of E. P. Roe\\n<p>In sending this,...   \n",
       "866  <p>'It's very good of you to have met me, Ambr...   \n",
       "873  <p>\"The Forsyte Saga\" was the title originally...   \n",
       "989  <p>Edition: 10\\n<p>Juliet Sutherland, Charles ...   \n",
       "897  <p>E-text prepared by Mardi Desjardins, Stephe...   \n",
       "906  <p>A list of the changes made can be found at ...   \n",
       "990  <p>It was Sunday evening, and on Sundays Max S...   \n",
       "\n",
       "                           genre  \n",
       "961                     Literary  \n",
       "962                     Literary  \n",
       "995                     Literary  \n",
       "958                     Literary  \n",
       "959        Detective and Mystery  \n",
       "967                     Literary  \n",
       "968                     Literary  \n",
       "969                     Literary  \n",
       "376        Detective and Mystery  \n",
       "741                     Literary  \n",
       "834              Western Stories  \n",
       "841                     Literary  \n",
       "862            Sea and Adventure  \n",
       "988        Detective and Mystery  \n",
       "866        Detective and Mystery  \n",
       "873  Humorous and Wit and Satire  \n",
       "989                     Literary  \n",
       "897                     Literary  \n",
       "906                     Literary  \n",
       "990                     Literary  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting it in dataframe\n",
    "df = pd.DataFrame.from_dict(Content,orient='index').reset_index()\n",
    "#Renaming column names\n",
    "df.columns = ['File_Name', 'Data']\n",
    "#Adding FilNo column to join\n",
    "df['FileNo'] = df['File_Name'].str.replace(r'\\D+', '').astype(int)\n",
    "df.sort_values(by=['FileNo'], inplace=True)\n",
    "data['Book_Data'] = df.Data\n",
    "data\n",
    "books = pd.DataFrame({'Data':data.Book_Data[0:20], 'genre':data.guten_genre[0:20]})\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is to see number of books per authors, but so many authors hence useless\n",
    "#ax = sns.barplot(x = books['Author_Name'], y = books['FileNo'])\n",
    "\n",
    "#Plotting number of authors per gerne\n",
    "import matplotlib.pyplot as plt\n",
    "books.groupby('genre')['Data'].count().plot(kind = 'bar')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a text to a sequence of words.\n",
    "def content_wordlist(Data, remove_stopwords=False):\n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(Data).get_text()\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits a Data into sentences\n",
    "def book_content(Data, tokenizer, remove_stopwords=False):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(Data.strip())\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(content_wordlist(raw_sentence,\\\n",
    "                                            remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for Data in books[\"Data\"]:\n",
    "    sentences += book_content(Data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the built-in logging module\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-26 11:19:07,360 : INFO : collecting all words and their counts\n",
      "2020-05-26 11:19:07,366 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-05-26 11:19:07,441 : INFO : PROGRESS: at sentence #10000, processed 213206 words, keeping 10890 word types\n",
      "2020-05-26 11:19:07,493 : INFO : PROGRESS: at sentence #20000, processed 373558 words, keeping 15071 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-26 11:19:07,577 : INFO : PROGRESS: at sentence #30000, processed 623576 words, keeping 19982 word types\n",
      "2020-05-26 11:19:07,643 : INFO : PROGRESS: at sentence #40000, processed 831356 words, keeping 22675 word types\n",
      "2020-05-26 11:19:07,685 : INFO : PROGRESS: at sentence #50000, processed 997140 words, keeping 24820 word types\n",
      "2020-05-26 11:19:07,732 : INFO : PROGRESS: at sentence #60000, processed 1158859 words, keeping 26249 word types\n",
      "2020-05-26 11:19:07,786 : INFO : PROGRESS: at sentence #70000, processed 1323949 words, keeping 28028 word types\n",
      "2020-05-26 11:19:07,838 : INFO : PROGRESS: at sentence #80000, processed 1460330 words, keeping 29121 word types\n",
      "2020-05-26 11:19:07,897 : INFO : collected 31828 word types from a corpus of 1683977 raw words and 89541 sentences\n",
      "2020-05-26 11:19:07,898 : INFO : Loading a fresh vocabulary\n",
      "2020-05-26 11:19:07,928 : INFO : effective_min_count=40 retains 3215 unique words (10% of original 31828, drops 28613)\n",
      "2020-05-26 11:19:07,929 : INFO : effective_min_count=40 leaves 1520821 word corpus (90% of original 1683977, drops 163156)\n",
      "2020-05-26 11:19:07,948 : INFO : deleting the raw counts dictionary of 31828 items\n",
      "2020-05-26 11:19:07,951 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2020-05-26 11:19:07,953 : INFO : downsampling leaves estimated 1051574 word corpus (69.1% of prior 1520821)\n",
      "2020-05-26 11:19:07,969 : INFO : estimated required memory for 3215 words and 300 dimensions: 9323500 bytes\n",
      "2020-05-26 11:19:07,970 : INFO : resetting layer weights\n",
      "2020-05-26 11:19:08,025 : INFO : training model with 4 workers on 3215 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-05-26 11:19:09,126 : INFO : EPOCH 1 - PROGRESS: at 93.91% examples, 967208 words/s, in_qsize 7, out_qsize 0\n",
      "2020-05-26 11:19:09,202 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-26 11:19:09,206 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-26 11:19:09,224 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-26 11:19:09,225 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-26 11:19:09,225 : INFO : EPOCH - 1 : training on 1683977 raw words (1050904 effective words) took 1.1s, 952656 effective words/s\n",
      "2020-05-26 11:19:10,246 : INFO : EPOCH 2 - PROGRESS: at 92.12% examples, 934236 words/s, in_qsize 7, out_qsize 0\n",
      "2020-05-26 11:19:10,356 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-26 11:19:10,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-26 11:19:10,371 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-26 11:19:10,375 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-26 11:19:10,376 : INFO : EPOCH - 2 : training on 1683977 raw words (1051595 effective words) took 1.1s, 919237 effective words/s\n",
      "2020-05-26 11:19:11,388 : INFO : EPOCH 3 - PROGRESS: at 58.21% examples, 641112 words/s, in_qsize 7, out_qsize 0\n",
      "2020-05-26 11:19:12,065 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-26 11:19:12,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-26 11:19:12,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-26 11:19:12,102 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-26 11:19:12,105 : INFO : EPOCH - 3 : training on 1683977 raw words (1051812 effective words) took 1.7s, 612095 effective words/s\n",
      "2020-05-26 11:19:13,139 : INFO : EPOCH 4 - PROGRESS: at 54.40% examples, 608773 words/s, in_qsize 7, out_qsize 0\n",
      "2020-05-26 11:19:13,690 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-26 11:19:13,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-26 11:19:13,717 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-26 11:19:13,718 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-26 11:19:13,721 : INFO : EPOCH - 4 : training on 1683977 raw words (1051415 effective words) took 1.6s, 663825 effective words/s\n",
      "2020-05-26 11:19:14,746 : INFO : EPOCH 5 - PROGRESS: at 65.82% examples, 699664 words/s, in_qsize 8, out_qsize 0\n",
      "2020-05-26 11:19:15,313 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-26 11:19:15,334 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-26 11:19:15,336 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-26 11:19:15,340 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-26 11:19:15,341 : INFO : EPOCH - 5 : training on 1683977 raw words (1051458 effective words) took 1.6s, 653412 effective words/s\n",
      "2020-05-26 11:19:15,342 : INFO : training on a 8419885 raw words (5257184 effective words) took 7.3s, 718737 effective words/s\n",
      "2020-05-26 11:19:15,355 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-05-26 11:19:15,375 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2020-05-26 11:19:15,379 : INFO : not storing attribute vectors_norm\n",
      "2020-05-26 11:19:15,382 : INFO : not storing attribute cum_table\n",
      "2020-05-26 11:19:15,493 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"word2vec_Model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gentleman', 0.8170490264892578),\n",
       " ('fellow', 0.7655470371246338),\n",
       " ('merchant', 0.7195016145706177),\n",
       " ('woman', 0.7087341547012329),\n",
       " ('jolyon', 0.6422972083091736),\n",
       " ('fool', 0.6112238168716431),\n",
       " ('person', 0.6029847860336304),\n",
       " ('creature', 0.5664562582969666),\n",
       " ('lady', 0.560076117515564),\n",
       " ('artist', 0.5571185350418091)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will print the most similar words present in the model\n",
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('odd', 0.7660984992980957),\n",
       " ('adventure', 0.6948403120040894),\n",
       " ('immense', 0.677749514579773),\n",
       " ('unusual', 0.6552022695541382),\n",
       " ('unexpected', 0.6430626511573792),\n",
       " ('event', 0.6363199353218079),\n",
       " ('angel', 0.6167119741439819),\n",
       " ('instant', 0.6147767305374146),\n",
       " ('excellent', 0.6096200346946716),\n",
       " ('awkward', 0.5986032485961914)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3215, 300)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will give the total number of words in the vocabolary created from this dataset\n",
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(Data, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(books.Data),num_features),dtype=\"float32\")\n",
    "    for Data in books.Data:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Data %d of %d\"%(counter,len(books.Data)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(Data, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 0 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for training set\n",
    "clean_train_reviews = []\n",
    "for Data in books['Data']:\n",
    "    clean_train_reviews.append(content_wordlist(Data, remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(trainDataVecs, books.genre, test_size = .5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 200)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score before any preprocessing is :0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         2\n",
      "           2       0.86      0.86      0.86         7\n",
      "           3       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.40      0.45      0.42        10\n",
      "weighted avg       0.67      0.70      0.68        10\n",
      "\n",
      "[[1 1 0]\n",
      " [1 6 0]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "c_score = forest.fit(X_train, y_train).score(X_test,y_test) \n",
    "print(\"Score before any preprocessing is :\"+str(c_score))\n",
    "\n",
    "#show the confusin matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test, result)) \n",
    "cm = confusion_matrix(y_test, result) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
